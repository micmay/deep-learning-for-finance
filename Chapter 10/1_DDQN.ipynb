{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "# Defining the Double Deep Q-Network agent\n",
        "class DDQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size  = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory  = []      # For experience replay\n",
        "        self.gamma   = 0.9     # Discount factor\n",
        "        self.epsilon = 0.001   # Exploration rate\n",
        "        self.epsilon_min   = 0.001 # Minimum value of epsilon\n",
        "        self.epsilon_decay = 0.9   # Gradually reducing the exploration factor\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = keras.Sequential() # Create a linear stack of layers\n",
        "        model.add(keras.layers.Dense(6, input_dim = self.state_size, activation = 'relu')) # Add a fully connected layer\n",
        "        model.add(keras.layers.Dense(6, activation = 'relu'))\n",
        "        model.add(keras.layers.Dense(self.action_size, activation = 'linear'))\n",
        "        model.compile(loss = 'mse', optimizer = keras.optimizers.Adam(lr = 0.001))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        \n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        minibatch = [self.memory[idx] for idx in indices]\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                action_next = np.argmax(self.model.predict(next_state)[0])\n",
        "                target = reward + self.gamma * self.target_model.predict(next_state)[0][action_next]\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        \n",
        "# Generate the Sinewave time series\n",
        "time_steps = 500 # Length of the series\n",
        "data = np.sin(np.arange(0, 10 * np.pi, 10 * np.pi / time_steps))\n",
        "\n",
        "# Preprocess the data\n",
        "window_size = 5\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(len(data) - window_size):\n",
        "    X.append(data[i:i + window_size])\n",
        "    Y.append(data[i + window_size])\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# Set up the DDQN agent\n",
        "state_size = window_size\n",
        "action_size = 1\n",
        "agent = DDQNAgent(state_size, action_size)\n",
        "\n",
        "# Train the DDQN agent\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "update_target_freq = 10\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(X) - 1):\n",
        "        state = np.reshape(X[i], [1, state_size])\n",
        "        action = agent.act(state)\n",
        "        next_state = np.reshape(X[i + 1], [1, state_size])\n",
        "        reward = Y[i]\n",
        "        done = False\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "    if len(agent.memory) > batch_size:\n",
        "        agent.replay(batch_size)\n",
        "\n",
        "    if epoch % update_target_freq == 0:\n",
        "        agent.update_target_model()\n",
        "\n",
        "# Test the trained model\n",
        "test_data = np.sin(np.arange(0, 12 * np.pi, 10 * np.pi / 5000))\n",
        "test_X = []\n",
        "\n",
        "for i in range(len(test_data) - window_size):\n",
        "    test_X.append(test_data[i:i + window_size])\n",
        "test_X = np.array(test_X)\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(test_X)):\n",
        "    state = np.reshape(test_X[i], [1, state_size])\n",
        "    prediction = agent.model.predict(state)[0][0]\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(np.arange(len(test_data)), test_data, label = 'Actual')\n",
        "plt.plot(np.arange(window_size, len(test_data)), predictions, label = 'Predicted', linestyle = 'dashed')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.grid()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}